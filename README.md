# Udacity-Data-Scientist-Nanodegree
A repository containing projects I created in the Udacity Data Science Nanodegree

# Project 1: A Data Science Blog Post
I analyzed a dataset from Stack Overflows 2020 Developer Survey data using the CRISP-DM process. The goal was to answer possible business questions. The projects includes an IIpython notebook and .html file with my analysis and questions. This culminated in writing blog post which you can find here: The link [Carve Yourself A Path Into Software Development](https://medium.com/@kehindeadiguno/carve-yourself-a-path-into-software-development-dbeaa78901de)

# Project 2: Disaster Response Pipeline
I applied my data engineering skills to analyze disaster data from Figure Eight to build a model for an API that classifies disaster messages. I created a machine learning pipeline to categorize real messages that were sent during disaster events so that the messages could be sent to an appropriate disaster relief agency. The project includes a web app where an emergency worker can input a new message and get classification results in several categories. The web app also displays visualizations of the data.

# Project 3: Making Recommendations with IBM
In this project, I worked on making article recommendations using interactions that users have with articles on the IBM Watson Studio platform. The project worked in three distinct sections. The sections included an analysis of the provided dataset and making recommendations to customers about articles they might like using three techniques. These include User-User Based Collaborative Filtering, matrix factorization and Content Based Recommendation using Natural Language Processing. The last section, developing an application, is still in progress.

# Project 4: Churn Prediction With Pyspark and Sparkify
In this project, I tackle an important business problem, predicting customer churn. I walk through the process of analysing and manipulating a large dataset with PySpark, engineering features, and training a model to predict churn with the Spark ML library. Spark is a distributed data processing engine built to process big data. And when we have big data, traditional libraries, like Scikit-Learn, that load all data in memory are generally unsuitable. I also wrote a blog post on this process which you can find here: The link [Churn Prediction with Sparkify](https://medium.com/@kehindeadiguno/analysing-customer-churn-with-pyspark-b23329e4525c)


